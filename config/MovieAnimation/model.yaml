transformer:
  encoder_layer: 16
  encoder_head: 8
  encoder_hidden: 256
  decoder_layer: 16
  decoder_head: 8
  decoder_hidden: 256
  conv_filter_size: 1024
  conv_kernel_size: [9, 1]
  encoder_dropout: 0.2
  decoder_dropout: 0.2
  pre_net_bottleneck: True
  style_FFT: True
  style_dim: 256
  face_id: False
  encoder_style: True
  use_mel_style_encoder: False

Lip_transformer:
  encoder_layer: 8
  encoder_head: 8
  encoder_hidden: 512
  decoder_hidden: 512
  conv_filter_size: 1024
  conv_kernel_size: [9, 1]
  encoder_dropout: 0.2

loss_function:
  model: "V2C"
  speaker_loss: False

classifier:  
  cls_hidden: []
  
upsample_ConvTranspose:
  resblock: 1
  upsample_rates: [2, 2] # [8,5,2,2]
  upsample_kernel_sizes: [4, 4] # [16,10,4,4]
  upsample_initial_channel: 256
  resblock_kernel_sizes: [ 3,7,11 ]
  resblock_dilation_sizes: [ [ 1,3,5 ], [ 1,3,5 ], [ 1,3,5 ] ]

variance_predictor:
  filter_size: 256
  kernel_size: 3
  dropout: 0.5
  predictor: False

variance_embedding:
  pitch_quantization: "linear" # support 'linear' or 'log', 'log' is allowed only if the pitch values are not normalized during preprocessing
  energy_quantization: "linear" # support 'linear' or 'log', 'log' is allowed only if the energy values are not normalized during preprocessing
  n_bins: 256

Enhancement:
  Content_enhancement: False
  Identity_enhancement: False

melencoder:
  encoder_hidden: 256  # 256 only for multi-speaker Fastspeech2 otherwise 128
  spectral_layer: 2
  temporal_layer: 2
  slf_attn_layer: 1
  slf_attn_head: 2
  conv_kernel_size: 5
  encoder_dropout: 0.1

multi_speaker: True
with_emotion: True

Symbols:
  phonemenumber: 87

learn_speaker: False # whether use Embedding layer to learn speaker embedding
learn_emotion: False # whether use Embedding layer to learn emotion embedding

max_seq_len: 1000

vocoder:
  model: "realHiFi-GAN_UniverVersion" # support 'HiFi-GAN', 'MelGAN', 'HiFi_GAN_16', "ISTFTNET", "HiFi_GAN_220"
  speaker: "LJSpeech" # support  'LJSpeech', 'universal', 'LJSpeech_16KHz'
  vocoder_checkpoint_path: "hifigan"

Multi_head_Duration_Aligner:
  Multi-Head_Attention: Ture
  Expand_with_Conv-Transpose: Ture
  Fusion_in_advance: False
  ResNet_multi-scales: False
  Duration_Search: False
  

Affective_Prosody_Adaptor:
  Embedding_Augmentation_in_pitch: Ture
  Embedding_Augmentation_in_energy: Ture
  Add_energy_valence: Ture  # Add or Concat
  cascade: Ture
  Use_Scale_attention: True

# train_model: 'pretrain' :TTS Pre-train Task in First-stage
# train_mode: 'other'  : Train without Multi-task Pre-train
# train_mode: 'finetune'  : Train with Multi-task Pre-train

train_mode: 'finetune'
pretrain_ckpt_path: 'output/official/ckpt/50000.pth.tar'

